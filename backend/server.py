# ==============================================================================
# --- INITIAL SETUP & IMPORTS ---
# ==============================================================================

# This script acts as a Flask-based backend server to handle API requests 
# and WebSocket events from the AIDA project's frontend.

# Standard Library Imports
import base64
import csv
import getpass
import io
import os
import shlex
import shutil
import time
import logging
import subprocess
import tempfile
import uuid
import zipfile
from datetime import datetime
import threading
import glob
import re

# Third-party Library Imports
import pandas as pd
import psutil
from flask import Flask, jsonify, make_response, request, send_file, send_from_directory
from flask_cors import CORS
from flask_socketio import SocketIO, emit

#  Flask App Initialization 
app = Flask(__name__)
CORS(app)  # Enable Cross-Origin Resource Sharing for the app.
socketio = SocketIO(app, cors_allowed_origins="*", 
                    async_mode='eventlet', 
                    ping_timeout=3600, 
                    ping_interval=25)  # Initialize Socket.IO with CORS for real-time communication.
logging.basicConfig(level=logging.DEBUG)  # Configure basic logging for debugging purposes.

# Base fixed path for all the data-folders inside the container
DATA_BASE_PATH = '/data'

# Directory Configuration 
# Define a dictionary of parent directories used throughout the application.
# This centralizes path management, making the script easier to maintain.
PARENT_DIRS = {
    'parent_dir_dataset': 'parent_dir_dataset',     # Stores all datasets generated by Generator.py.
    'parent_dir_histogram': 'parent_dir_histogram', # Stores all histograms generated by extract_histograms.py.
    'indexes': 'indexes',                           # Stores spatial indexes for datasets.
    'parent_dir_input_ds': 'parent_dir_input_ds',   # Stores CSV files with parameters for Generator.py.
    'parent_dir_rq_input': 'parent_dir_rq_input',   # Stores input CSV files for Range Queries.
    'range_query_results': 'rangeQueriesResult',    # Stores the results of Range Queries.
    'parent_dir_rank': 'parent_dir_rank',           # Stores input files for the Rank & Diff analysis.
    'scala_project_root': 'scalaScript',            # Path to the root of the Scala scripts project.
    'trainingSets': 'trainingSets',                 # Stores files related to the balancing analysis (training sets).
    'augmentation_logs': 'augmentation_logs',       # Stores log files from the augmentation process.
    'datasetsAugmentation': 'datasetsAugmentation',  # Stores datasets generated by the augmentation process.
    'fractalDimension': 'fractalDimension'          # Stores fractal dimensions results.
}

# Directory Creation 
# Ensure that all defined parent directories exist on the filesystem.
# If a directory doesn't exist, it is created automatically.
for key, path in PARENT_DIRS.items():
    # Build the complete path using DATA_BASE_PATH
    full_path = os.path.join(DATA_BASE_PATH, path)
    PARENT_DIRS[key] = full_path # Update dictionary with correct and full path
    os.makedirs(full_path, exist_ok=True)
# Local Script Imports 
# Import necessary functions from the local 'extract_histograms.py' script.
# This is needed because the script has a fixed input/output structure.
try:
    from extract_histograms import extract_histogram
except ImportError:
    logging.error("Could not import 'extract_histogram'. Make sure 'extract_histograms.py' is in the root folder.")
    exit(1) 

# ==============================================================================
# --- SOCKET.IO CONNECTION HANDLING ---
# ==============================================================================

@socketio.on('connect')
def test_connect():
    """Handles a new client connection event for logging purposes."""
    logging.debug('Client connected to the server.')

# ==============================================================================
# --- HELPER FUNCTION FOR RESOURCE MONITORING ---
# ==============================================================================

def monitor_process_and_emit_usage(process, stop_event):
    """
    Monitors a specific subprocess's CPU and RAM usage in a separate thread.
    It periodically emits 'resource_usage' events to the client via Socket.IO
    until the provided stop_event is set or the process ends.

    Args:
        process (subprocess.Popen): The subprocess to monitor.
        stop_event (threading.Event): An event object to signal when the thread should stop.
    """
    logging.debug(f"Process-specific monitoring thread started for PID: {process.pid}.")
    try:
        p = psutil.Process(process.pid)
        total_ram = psutil.virtual_memory().total
        cpu_count = psutil.cpu_count() or 1 # Avoid division by zero
        
        # Prime the function before the loop. The first call returns 0.0 but sets a baseline.
        p.cpu_percent(interval=None)

        while not stop_event.is_set():
            # Check if the process is still running
            if not p.is_running():
                break
                
            # Get current CPU and RAM usage for the specific process.
            # Normalizing by CPU count gives a more intuitive percentage of the total system capacity.
            cpu_percent = p.cpu_percent(interval=1) / cpu_count
            
            # Get RAM usage (RSS - Resident Set Size) and calculate percentage.
            ram_rss = p.memory_info().rss
            ram_percent = (ram_rss / total_ram) * 100 if total_ram > 0 else 0
            
            # Emit the data to all connected clients.
            socketio.emit('resource_usage', {'cpu': cpu_percent, 'ram': ram_percent})
            
            # Use socketio.sleep for cooperative multitasking.
            socketio.sleep(1)
    except psutil.NoSuchProcess:
        logging.debug(f"Process {process.pid} no longer exists. Monitoring stopped.")
    except Exception as e:
        logging.error(f"Error in monitoring thread for PID {process.pid}: {e}")
    finally:
        # Emit a final zero-usage event to reset the gauge on the frontend.
        socketio.emit('resource_usage', {'cpu': 0, 'ram': 0})
        logging.debug(f"Process-specific monitoring thread stopped for PID: {process.pid}.")

# User mainly for histogram API due to different method of usage/call
def monitor_and_emit_usage(stop_event):
    """
    Monitors system-wide CPU and RAM usage in a separate thread.
    It periodically emits 'resource_usage' events to the client via Socket.IO
    until the provided stop_event is set.

    Args:
        stop_event (threading.Event): An event object to signal when the thread should stop.
    """
    logging.debug("Resource monitoring thread started.")
    while not stop_event.is_set():
        # Get current CPU and RAM usage percentages.
        cpu_percent = psutil.cpu_percent(interval=1)
        ram_percent = psutil.virtual_memory().percent
        
        # Emit the data to all connected clients.
        socketio.emit('resource_usage', {'cpu': cpu_percent, 'ram': ram_percent})
        
        # Use socketio.sleep for cooperative multitasking within the event loop.
        socketio.sleep(1) 
    logging.debug("Resource monitoring thread stopped.")

# ==============================================================================
# --- API ENDPOINTS FOR rank_with_diff.py SCRIPT ---
# ==============================================================================

@socketio.on('run_rank_diff')
def run_rank_diff_socket(data):
    """
    Handles the 'run_rank_diff' event from the client to execute the balancing analysis script.
    It constructs a configuration CSV, runs the Python script as a subprocess, and communicates
    the results or errors back to the client via Socket.IO.

    Args:
        data (dict): A dictionary containing parameters from the frontend:
                     - paramToCategorize: The parameter to use for categorization (e.g., 'cardinality').
                     - numberIntervals: The number of intervals for ranking.
                     - rqResultFile: The name of the range query result file to analyze.
    """
    logging.debug('Received run_rank_diff event via Socket.IO')
    
    stop_monitoring_event = threading.Event()

    try:
        # --- 1. Parameter Extraction and Validation ---
        param_to_categorize = data.get('paramToCategorize')
        number_intervals = data.get('numberIntervals')
        rq_results_folder_name = data.get('rqSessionFolder', '') 
        rq_result_file_name = data.get('rqResultFile')

        if not all([param_to_categorize, number_intervals, rq_result_file_name]):
            emit('rank_diff_error', {'error': 'Missing required parameters.'})
            return
        if not isinstance(number_intervals, int) or number_intervals <= 0:
            emit('rank_diff_error', {'error': 'numberIntervals must be a positive integer.'})
            return

        # --- 2. Path Construction and File Verification ---
        path_range_queries_result = os.path.join(PARENT_DIRS['range_query_results'], rq_results_folder_name)
        specific_rq_result_full_path = os.path.join(path_range_queries_result, rq_result_file_name)

        if not os.path.exists(specific_rq_result_full_path):
            emit('rank_diff_error', {'error': f"Specified RQ result file not found at: {specific_rq_result_full_path}"})
            return
        
        # Use regex to robustly extract the unique session ID
        # The session ID is defined as YYYYMMDD_HHMMSS_UUID. This avoids dependency on prefixes like 'rqR_'.
        session_id_match = re.search(r'(\d{8}_\d{6}_[0-9a-fA-F]{8})', rq_result_file_name)
        if not session_id_match:
            error_msg = f"Could not extract a valid session ID (e.g., YYYYMMDD_HHMMSS_UUID) from filename: {rq_result_file_name}"
            emit('rank_diff_error', {'error': error_msg})
            return
        session_id = session_id_match.group(1)
        logging.debug(f"Extracted session ID: {session_id}")
        
        # Construct the summary filename using the extracted session ID.
        path_summaries_dir = PARENT_DIRS['parent_dir_input_ds']
        name_summary_file = f"input_params_collection_{session_id}.csv"
        
        # --- Dynamically Find Fractal Dimension Files using the extracted ID ---
        path_fd_dir = PARENT_DIRS['fractalDimension']
        found_fd_files = []
        if os.path.isdir(path_fd_dir):
            for filename in os.listdir(path_fd_dir):
                if session_id in filename:
                    found_fd_files.append(filename)
                    logging.debug(f"Found matching Fractal Dimension file: {filename}")
        
        # --- 3. Dynamic Configuration File Generation ---
        rank_params_csv_path = os.path.join(DATA_BASE_PATH, "rankParameters.csv")
        header = ['parameterCategorized', 'numberIntervals', 'pathRangeQueriesResult', 'nameRangeQueriesResult', 'pathSummaries', 'nameSummary', 'pathFD', 'nameFD']
        
        row_values = [
            param_to_categorize, number_intervals,
            path_range_queries_result, rq_result_file_name,
            path_summaries_dir, name_summary_file,
            path_fd_dir,
        ] + found_fd_files

        with open(rank_params_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile, delimiter=';')
            writer.writerow(header) 
            writer.writerow(row_values)
        logging.debug(f"Created rankParameters.csv at: {rank_params_csv_path}")

        # --- 4. Subprocess Execution ---
        current_script_dir = os.path.dirname(os.path.abspath(__file__))
        rank_diff_script_path = os.path.join(current_script_dir, "rank_with_diff.py")
        cmd = ["python", rank_diff_script_path]
        logging.debug(f"Executing command: {' '.join(cmd)} in CWD: {DATA_BASE_PATH}")

        socketio.start_background_task(monitor_and_emit_usage, stop_monitoring_event)

        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, cwd=DATA_BASE_PATH, encoding='utf-8', bufsize=1)
        
        # --- 5. Real-time Output Streaming and Processing ---
        stdout_lines = []
        for line in iter(process.stdout.readline, ''):
            logging.info(f"rank_with_diff.py output: {line.strip()}")
            stdout_lines.append(line)
            socketio.sleep(0)

        process.wait()
        stdout = "".join(stdout_lines)

        # --- 6. Result Handling and Client Communication ---
        if process.returncode != 0:
            logging.error(f"Error executing rank_with_diff.py. STDOUT: {stdout}")
            error_details = f"Error running rank_with_diff.py."
            emit('rank_diff_error', {'error': error_details, 'output': stdout})
        else:
            logging.info(f"rank_with_diff.py executed successfully. Output: {stdout}")
            output_folder_leaf = re.sub(r'^(rqR_)?(.*?)(_ts\d*)?$', r'\2', rq_result_file_name.rsplit('.', 1)[0])
            output_info = f"Output files have been generated under the 'trainingSets/{output_folder_leaf}' directory."
            emit('rank_diff_complete', {'message': 'Rank and difference sets generated successfully.', 'details': output_info, 'output': stdout})

    except Exception as e:
        logging.exception("Exception during rank_with_diff.py execution.")
        emit('rank_diff_error', {'error': f'Internal server error: {str(e)}'})
    finally:
        # --- 7. Cleanup ---
        stop_monitoring_event.set()

# ==============================================================================
# --- API ENDPOINTS FOR SCALA PROGRAMS (Range Query) ---
# ==============================================================================

def find_input_params_csv(dataset_folder_basename, input_ds_path):
    """
    Helper function to find the corresponding 'input_params_*.csv' file for a given dataset folder.
    It searches for a file that contains the dataset folder's base name.

    Args:
        dataset_folder_basename (str): The base name of the dataset folder (e.g., 'dataset_20250616_011214_9fd6e7e1').
        input_ds_path (str): The directory where the input_params files are stored.

    Returns:
        str: The full path to the found CSV file, or None if not found.
    """
    for filename in os.listdir(input_ds_path):
        if filename.startswith("input_params_") and dataset_folder_basename in filename and filename.endswith(".csv"):
            return os.path.join(input_ds_path, filename)
    return None

def _run_queries_thread(data):
    """
    Runs the Scala RangeQueryApp for manually entered queries in a background thread.
    This function contains the long-running logic: it prepares configuration files,
    iterates through datasets, executes the Scala subprocess for each, and emits
    progress and completion events via Socket.IO.

    Args:
        data (dict): The original data dictionary from the 'generate_queries' event.
    """
    stop_monitoring_event = threading.Event()

    try:
        # Extract Data and Set Up Paths
        queries_from_frontend = data.get('queries', [])
        selected_folder_name = data.get('folder')

        sbt_executable_name = "sbt"
        rqsbt_project_root = PARENT_DIRS['scala_project_root']
        dataset_base_path = PARENT_DIRS['parent_dir_dataset']
        index_base_path = PARENT_DIRS['indexes']
        rq_input_base_path = PARENT_DIRS['parent_dir_rq_input']
        
        parent_dataset_folder_path = os.path.join(dataset_base_path, selected_folder_name)
        if not os.path.isdir(parent_dataset_folder_path):
            socketio.emit('generate_query_error', {'error': f"Dataset folder '{selected_folder_name}' not found."})
            return

        dataset_files = [f for f in os.listdir(parent_dataset_folder_path) if os.path.isfile(os.path.join(parent_dataset_folder_path, f)) and f.lower().endswith(('.csv', '.wkt', '.geojson'))]
        if not dataset_files:
            socketio.emit('generate_query_error', {'error': f"No dataset files found in folder '{selected_folder_name}'."})
            return

        # Create a temporary CSV for query inputs
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = uuid.uuid4().hex[:8]
        query_input_csv_filename = f"rq_input_queries_manual_{os.path.basename(selected_folder_name)}_{timestamp}_{unique_id}.csv"
        query_input_csv_path = os.path.abspath(os.path.join(rq_input_base_path, query_input_csv_filename))

        csv_columns_for_scala = ["queryDatasetName", "numQuery", "queryArea", "minX", "minY", "maxX", "maxY", "areaint"]
        with open(query_input_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_columns_for_scala, delimiter=';')
            writer.writeheader()
            for q in queries_from_frontend:
                writer.writerow({
                    "queryDatasetName": q.get('datasetName', ''), "numQuery": q.get('numQuery', ''),
                    "queryArea": q.get('queryArea', ''), "minX": q.get('minX', ''), "minY": q.get('minY', ''),
                    "maxX": q.get('maxX', ''), "maxY": q.get('maxY', ''), "areaint": q.get('areaint', '')
                })

        # Find corresponding summary file
        mbr_summary_file_path = find_input_params_csv(os.path.basename(selected_folder_name), PARENT_DIRS['parent_dir_input_ds'])
        if not mbr_summary_file_path or not os.path.exists(mbr_summary_file_path):
             socketio.emit('generate_query_error', {'error': f"Corresponding MBR summary file not found for folder '{selected_folder_name}'."})
             if os.path.exists(query_input_csv_path): os.remove(query_input_csv_path)
             return

        # Filter datasets to process based on query definitions
        datasets_in_query_file = {q.get('datasetName') for q in queries_from_frontend}
        datasets_to_process_actually = [df for df in dataset_files if os.path.splitext(df)[0] in datasets_in_query_file]
        if not datasets_to_process_actually:
            socketio.emit('generate_query_error', {'error': 'None of the datasets in the folder are mentioned in the queries.'})
            os.remove(query_input_csv_path)
            return
        
        # Process each dataset
        has_errors = False
        range_params_csv_path = os.path.join(rqsbt_project_root, "rangeParameters.csv")
        range_params_header = ["pathDatasets", "nameDataset", "pathSummaries", "nameSummary", "pathIndexes", "pathRangeQueries", "nameRangeQueries"]
        total_datasets_to_process = len(datasets_to_process_actually)
        
        for i, dataset_file_name in enumerate(datasets_to_process_actually):
            single_dataset_name_without_ext = os.path.splitext(dataset_file_name)[0]
            
            with open(range_params_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=range_params_header, delimiter=';')
                writer.writeheader()
                writer.writerow({
                    "pathDatasets": parent_dataset_folder_path, "nameDataset": single_dataset_name_without_ext,
                    "pathSummaries": PARENT_DIRS['parent_dir_input_ds'], "nameSummary": os.path.basename(mbr_summary_file_path),
                    "pathIndexes": os.path.join(index_base_path, selected_folder_name, single_dataset_name_without_ext + "_spatialIndex"),
                    "pathRangeQueries": rq_input_base_path, "nameRangeQueries": os.path.basename(query_input_csv_path)
                })

            scala_cmd = [sbt_executable_name, "runMain RangeQueryApp"]
            
            # Create a local stop event for this specific subprocess
            single_process_stop_event = threading.Event()
            process = subprocess.Popen(
                scala_cmd, cwd=rqsbt_project_root,
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                text=True, encoding='utf-8', bufsize=1
            )
            
            # Start monitoring specifically for this process
            socketio.start_background_task(monitor_process_and_emit_usage, process, single_process_stop_event)
            
            # Stream the process output without blocking
            for line in process.stdout:
                logging.info(f"Scala output ({single_dataset_name_without_ext}): {line.strip()}")
                socketio.sleep(0)  # Yield control to allow other tasks (like sending emits) to run
            
            process.wait()
            # Stop the monitoring for the completed process
            single_process_stop_event.set()
            
            if process.returncode != 0:
                has_errors = True
            
            socketio.emit('range_query_progress', {
                'current_dataset': dataset_file_name, 'processed_count': i + 1, 
                'total_count': total_datasets_to_process, 
                'progress': int(((i + 1) / total_datasets_to_process) * 100)
            })

        # Finalize and notify client with the final result
        output_filename = f"rqR_{selected_folder_name}.csv"
        if has_errors:
            socketio.emit('generate_query_complete', {'status': 'partial_success', 'output_folder_name': output_filename})
        else:
            socketio.emit('generate_query_complete', {'status': 'success', 'output_folder_name': output_filename})

    except Exception as e:
        logging.exception(f'Exception in _run_queries_thread: {e}')
        socketio.emit('generate_query_error', {'error': str(e)})

    finally:
        stop_monitoring_event.set()

@socketio.on('generate_queries')
def generate_queries_backend(data):
    """
    Handles the 'generate_queries' event to run the Scala RangeQueryApp.
    It immediately acknowledges the request and starts the long-running query
    process in a background thread to keep the server responsive.

    Args:
        data (dict): A dictionary from the frontend containing:
                     - queries: A list of query parameter dictionaries.
                     - folder: The name of the dataset folder to run queries on.
    
    Returns:
        dict: A confirmation that the process has been started. This is used
              as a callback on the client-side.
    """
    logging.debug('Received generate_queries event, starting background task.')
    socketio.start_background_task(_run_queries_thread, data)
    return {'status': 'started'}

def _run_queries_from_csv_thread(data):
    stop_monitoring_event = threading.Event()

    try:
        csv_file_b64 = data.get('csvFile')
        selected_folder_name = data.get('folder')
        
        sbt_executable_name = "sbt"
        rqsbt_project_root = PARENT_DIRS['scala_project_root']
        dataset_base_path = PARENT_DIRS['parent_dir_dataset']
        index_base_path = PARENT_DIRS['indexes']
        rq_input_base_path = PARENT_DIRS['parent_dir_rq_input']

        parent_dataset_folder_path = os.path.join(dataset_base_path, selected_folder_name)
        if not os.path.isdir(parent_dataset_folder_path):
            socketio.emit('generate_query_error', {'error': f"Dataset folder '{selected_folder_name}' not found."})
            return

        dataset_files = [f for f in os.listdir(parent_dataset_folder_path) if os.path.isfile(os.path.join(parent_dataset_folder_path, f)) and f.lower().endswith(('.csv', '.wkt', '.geojson'))]
        if not dataset_files:
            socketio.emit('generate_query_error', {'error': f"No dataset files found in folder '{selected_folder_name}'."})
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = uuid.uuid4().hex[:8]
        query_input_csv_filename = f"rq_input_queries_uploaded_{os.path.basename(selected_folder_name)}_{timestamp}_{unique_id}.csv"
        query_input_csv_path = os.path.abspath(os.path.join(rq_input_base_path, query_input_csv_filename))

        csv_file_content_b64 = csv_file_b64.split(',')[1]
        csv_file_bytes = base64.b64decode(csv_file_content_b64)
        csv_file_string = csv_file_bytes.decode('utf-8')
        with open(query_input_csv_path, 'w', newline='', encoding='utf-8') as f:
            f.write(csv_file_string)
        
        mbr_summary_file_path = find_input_params_csv(os.path.basename(selected_folder_name), PARENT_DIRS['parent_dir_input_ds'])
        if not mbr_summary_file_path or not os.path.exists(mbr_summary_file_path):
            socketio.emit('generate_query_error', {'error': f"Corresponding MBR summary file not found for '{selected_folder_name}'."})
            return

        datasets_in_query_file = set()
        with open(query_input_csv_path, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter=';')
            header = reader.fieldnames
            dataset_col_name = None
            if 'datasetName' in header: dataset_col_name = 'datasetName'
            elif 'queryDatasetName' in header: dataset_col_name = 'queryDatasetName'
            if not dataset_col_name: raise ValueError("Uploaded query CSV must contain a 'datasetName' or 'queryDatasetName' column.")
            for row in reader:
                if dataset_col_name in row and row[dataset_col_name]: datasets_in_query_file.add(row[dataset_col_name])
        
        datasets_to_process_actually = [df for df in dataset_files if os.path.splitext(df)[0] in datasets_in_query_file]
        if not datasets_to_process_actually:
            socketio.emit('generate_query_error', {'error': 'None of the datasets in the folder are mentioned in the uploaded file.'})
            return
        
        has_errors = False
        range_params_csv_path = os.path.join(rqsbt_project_root, "rangeParameters.csv")
        range_params_header = ["pathDatasets", "nameDataset", "pathSummaries", "nameSummary", "pathIndexes", "pathRangeQueries", "nameRangeQueries"]
        total_datasets_to_process = len(datasets_to_process_actually)

        # 1. Collect all dataset parameters for the multi-row CSV
        all_dataset_rows = []
        for dataset_file_name in datasets_to_process_actually:
            single_dataset_name_without_ext = os.path.splitext(dataset_file_name)[0]
            
            row_data = {
                "pathDatasets": parent_dataset_folder_path,
                "nameDataset": single_dataset_name_without_ext,
                "pathSummaries": PARENT_DIRS['parent_dir_input_ds'],
                "nameSummary": os.path.basename(mbr_summary_file_path),
                "pathIndexes": os.path.join(index_base_path, selected_folder_name, single_dataset_name_without_ext + "_spatialIndex"),
                "pathRangeQueries": rq_input_base_path,
                "nameRangeQueries": os.path.basename(query_input_csv_path)
            }
            all_dataset_rows.append(row_data)

        # 2. Write all datasets to rangeParameters.csv in a single operation
        with open(range_params_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=range_params_header, delimiter=';')
            writer.writeheader()
            writer.writerows(all_dataset_rows)
            
        # Emit starting progress
        socketio.emit('range_query_progress', {
            'current_dataset': "Starting analysis...", 'processed_count': 0, 
            'total_count': total_datasets_to_process, 
            'progress': 1
        })
        
        # 3. Execute the Scala App once
        scala_cmd = [sbt_executable_name, "runMain RangeQueryApp"]

        single_process_stop_event = threading.Event()
        process = subprocess.Popen(
            scala_cmd, cwd=rqsbt_project_root,
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
            text=True, encoding='utf-8', bufsize=1
        )
        
        socketio.start_background_task(monitor_process_and_emit_usage, process, single_process_stop_event)
        
        datasets_started = set()
        total_datasets_to_process = len(datasets_to_process_actually)
        
        for line in process.stdout:
            logging.info(f"Scala output: {line.strip()}")
            
            if "The dataset you want to analyze is" in line:
                try:
                    dataset_name_match = re.search(r"is '(.*?)'\. His geometry is", line)
                    
                    if dataset_name_match:
                        dataset_name = dataset_name_match.group(1)
                        if dataset_name not in datasets_started:
                            datasets_started.add(dataset_name)
                            current_processed_count = len(datasets_started)
                            
                            progress_percentage = int((current_processed_count / total_datasets_to_process) * 99) # Max 99%
                            
                            socketio.emit('range_query_progress', {
                                'current_dataset': f"Processing: {dataset_name} ({current_processed_count}/{total_datasets_to_process})", 
                                'processed_count': current_processed_count, 
                                'total_count': total_datasets_to_process, 
                                'progress': max(1, progress_percentage) 
                            })
                            
                except Exception as e:
                    logging.error(f"Error parsing Scala progress line: {e}")

            socketio.sleep(0)
                
        process.wait()
        single_process_stop_event.set()
        
        if process.returncode != 0:
            has_errors = True
        
        socketio.emit('range_query_progress', {
            'current_dataset': "All datasets processed", 
            'processed_count': total_datasets_to_process, 
            'total_count': total_datasets_to_process, 
            'progress': 100
        })
        
        output_filename = f"rqR_{selected_folder_name}.csv"
        if has_errors:
            socketio.emit('generate_query_complete', {'status': 'partial_success', 'output_folder_name': output_filename})
        else:
            socketio.emit('generate_query_complete', {'status': 'success', 'output_folder_name': output_filename})

    except Exception as e:
        logging.exception(f'Exception in _run_queries_from_csv_thread: {e}')
        socketio.emit('generate_query_error', {'error': str(e)})

    finally:
        stop_monitoring_event.set()

@socketio.on('generate_queries_from_csv')
def generate_queries_from_csv_backend(data):
    """
    Handles the 'generate_queries_from_csv' event to run the Scala RangeQueryApp.
    It acknowledges the request and starts the CSV processing and query execution
    in a background thread.

    Args:
        data (dict): A dictionary from the frontend containing:
                     - csvFile: A base64-encoded string of the uploaded CSV file.
                     - folder: The name of the dataset folder to run queries on.
    
    Returns:
        dict: A confirmation that the process has been started.
    """
    logging.debug('Received generate_queries_from_csv event, starting background task.')
    socketio.start_background_task(_run_queries_from_csv_thread, data)
    return {'status': 'started'}

# ==============================================================================
# --- API ENDPOINTS FOR SCALA PROGRAMS (Index)---
# ==============================================================================

def run_scala_indexing_and_emit(data, stop_monitoring_event):
    """
    Runs the Scala indexing process in a background thread and emits progress via Socket.IO.
    This version includes enhanced log parsing to provide detailed feedback during the
    SBT and application startup phases.

    Args:
        data (dict): Dictionary containing 'folder_name' and 'files_config'.
        stop_monitoring_event (threading.Event): Event to signal the monitoring thread to stop.
    """
    folder_name = data.get('folder_name')
    files_config = data.get('files_config', [])
    sbt_executable_name = "sbt"
    sbt_project_root = PARENT_DIRS['scala_project_root']

    try:
        # Prepare Paths 
        full_input_dataset_folder_path = os.path.join(PARENT_DIRS['parent_dir_dataset'], folder_name)
        specific_index_output_path = os.path.join(PARENT_DIRS['indexes'], folder_name)
        os.makedirs(specific_index_output_path, exist_ok=True)
        
        # Create Configuration CSV 
        index_params_csv_path_for_scala = os.path.join(sbt_project_root, "indexParameters.csv")
        csv_columns_for_scala = ["pathDatasets", "nameDataset", "pathIndexes", "typePartition", "num"]
        
        with open(index_params_csv_path_for_scala, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_columns_for_scala, delimiter=';')
            writer.writeheader()
            for file_cfg in files_config:
                writer.writerow({
                    "pathDatasets": full_input_dataset_folder_path, "nameDataset": file_cfg.get('fileName'),
                    "pathIndexes": specific_index_output_path, "typePartition": file_cfg.get('partitionType'),
                    "num": file_cfg.get('dimension')
                })
        logging.debug(f"indexParameters.csv created at: {index_params_csv_path_for_scala}")

        # Execute Scala Subprocess 
        cmd = [sbt_executable_name, "runMain IndexApp"]
        process = subprocess.Popen(
            cmd, cwd=sbt_project_root, stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT, text=True, encoding='utf-8', bufsize=1
        )

        socketio.start_background_task(monitor_process_and_emit_usage, process, stop_monitoring_event)
        
        # Stream Output and Handle Completion 
        processed_count = 0
        total_files = len(files_config)
        # Use a state variable to track the initialization phase and prevent duplicate messages.
        init_phase = 0 # 0=start, 1=sbt, 2=project, 3=running

        for line in process.stdout:
            line_stripped = line.strip()
            if not line_stripped: continue
            
            logging.info(f"Scala stdout: {line_stripped}")
            
            # --- Enhanced Status Parsing Logic ---
            # Phase 1: SBT and Scala Dependency Download
            if init_phase < 1 and ("getting org.scala-sbt" in line_stripped or "getting Scala" in line_stripped):
                socketio.emit('spatial_indexing_progress', {'progress': 5, 'message': 'Initializing SBT (this may take a moment)...', 'processed_count': 0, 'total_count': total_files})
                init_phase = 1
            # Phase 2: Loading Scala Project
            elif init_phase < 2 and "loading project" in line_stripped:
                socketio.emit('spatial_indexing_progress', {'progress': 10, 'message': 'Loading Scala project settings...', 'processed_count': 0, 'total_count': total_files})
                init_phase = 2
            # Phase 3: Starting the main application
            elif init_phase < 3 and "running (fork) IndexApp" in line_stripped:
                socketio.emit('spatial_indexing_progress', {'progress': 15, 'message': 'Starting Indexing Application...', 'processed_count': 0, 'total_count': total_files})
                init_phase = 3
            
            # --- Main Progress Parsing Logic ---
            # This logic triggers only after the initialization phases are complete.
            if init_phase >= 3 and "<System> Partitioning '" in line_stripped and "'!" in line_stripped:
                processed_count += 1
                try:
                    file_name = line_stripped.split("'")[1]
                    # Ensure progress doesn't go backwards from the initial setup values.
                    progress = max(15, int((processed_count / total_files) * 100))
                    socketio.emit('spatial_indexing_progress', {
                        'progress': progress, 'file_name': file_name,
                        'message': f"Indexing file {processed_count}/{total_files}: {file_name}",
                        'processed_count': processed_count, 'total_count': total_files
                    })
                    socketio.sleep(0)
                except (IndexError, ValueError):
                    pass
        
        process.wait()
        
        # Handle Final Status 
        if process.returncode != 0:
            logging.error(f"Error executing Scala IndexApp. Return code: {process.returncode}")
            socketio.emit('spatial_indexing_error', {'error': f"Indexing failed. Check backend logs for details."})
        else:
            logging.info("Scala IndexApp executed successfully.")
            socketio.emit('spatial_indexing_complete', {'message': f"Spatial indexing completed for all {total_files} files."})

    except Exception as e:
        logging.exception("Unexpected error during spatial indexing.")
        socketio.emit('spatial_indexing_error', {'error': f'Internal server error: {str(e)}'})
    finally:
        # Ensure the monitoring thread is always signaled to stop.
        stop_monitoring_event.set()


@app.route('/process_spatial_index', methods=['POST'])
def process_spatial_index_route():
    """
    API endpoint to start the spatial indexing process.
    """
    data = request.get_json()
    if not data.get('folder_name') or not data.get('files_config'):
        return jsonify({"error": "Missing folder_name or files_config."}), 400

    stop_monitoring_event = threading.Event()
    # Start the main indexing function in a background thread.
    socketio.start_background_task(run_scala_indexing_and_emit, data, stop_monitoring_event)

    return jsonify({"message": "Spatial indexing process started."}), 202

# ==============================================================================
# --- API ENDPOINTS FOR extract_histograms.py SCRIPT ---
# ==============================================================================

@socketio.on('process_histograms')
def process_histograms_socket(data):
    """
    Handles the 'process_histograms' event to generate histograms for all valid
    CSV files within a specified dataset folder. It iterates through the files,
    validates their format, runs the 'extract_histogram' function, and sends
    progress updates to the client.

    Args:
        data (dict): A dictionary from the frontend containing:
                     - folder_name: The name of the dataset folder to process.
    """
    folder_name = data.get('folder_name')
    logging.debug(f"Received process_histograms event for folder: {folder_name}")

    if not folder_name:
        emit('histogram_error', {'error': 'No folder name provided.'})
        return

    stop_monitoring_event = threading.Event()
    socketio.start_background_task(monitor_and_emit_usage, stop_monitoring_event)

    try:
        full_input_folder_path = os.path.join(PARENT_DIRS['parent_dir_dataset'], folder_name)

        if not os.path.isdir(full_input_folder_path):
            emit('histogram_error', {'error': f"Input folder: '{folder_name}' not found."})
            return
        
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        safe_folder_name = folder_name.replace(os.sep, '_').replace(' ', '_')
        output_sub_dir = f"{safe_folder_name}_histograms_{timestamp}"
        full_output_dir_path = os.path.join(PARENT_DIRS['parent_dir_histogram'], output_sub_dir)
        os.makedirs(full_output_dir_path, exist_ok=True)
        logging.debug(f"Created output directory for histograms: {full_output_dir_path}")

        all_items_in_folder = os.listdir(full_input_folder_path)
        csv_files_in_root = [f for f in all_items_in_folder if os.path.isfile(os.path.join(full_input_folder_path, f)) and f.endswith('.csv') and not f.startswith('.') and f not in ['dataset-summaries_ts.csv', 'rq_result_ts.csv', 'fd2_geom_allds_ts.csv', 'new_datasets.csv']]

        if not csv_files_in_root:
            emit('histogram_complete', {'message': f"No valid CSV files found in '{folder_name}' to process."})
            return

        num_rows, num_columns = 128, 128
        total_files = len(csv_files_in_root)
        processed_count = 0
        skipped_files = []

        for i, csv_file_name in enumerate(csv_files_in_root):
            input_file_path = os.path.join(full_input_folder_path, csv_file_name)
            
            progress = int(((i + 1) / total_files) * 100)
            emit('histogram_progress', {'progress': progress, 'file_name': csv_file_name, 'processed_count': i + 1, 'total_count': total_files})
            
            if not is_bounding_box_dataset(input_file_path):
                logging.info(f"Skipping non-bounding-box dataset: {input_file_path}")
                skipped_files.append(csv_file_name)
                # Yield control even if skipping, to keep UI responsive.
                socketio.sleep(0)
                continue

            output_file_name = csv_file_name.replace('.csv', '_summary.csv')
            output_file_path = os.path.join(full_output_dir_path, output_file_name)

            # Yield control to the server BEFORE the potentially blocking call.
            # This allows the resource monitor to update and keeps the UI responsive.
            socketio.sleep(0)

            logging.info(f"Processing: {input_file_path} -> {output_file_path}")
            extract_histogram(input_file_path, output_file_path, num_rows, num_columns)
            processed_count += 1
            
            # Yield control again AFTER the call to process the next item.
            socketio.sleep(0)

        message = f"Completed histogram extraction for {processed_count} files in '{folder_name}'. Output in: {output_sub_dir}"
        if skipped_files:
            message += f" Skipped {len(skipped_files)} non-bounding-box files."

        emit('histogram_complete', { "message": message })

    except Exception as e:
        logging.exception(f"Error during histogram extraction for folder: '{folder_name}'.")
        emit('histogram_error', {'error': f'Internal server error: {str(e)}'})
    finally:
        stop_monitoring_event.set()
    
def is_bounding_box_dataset(filepath):
    """
    Helper function to verify if a CSV file contains bounding box data.
    It checks if the first data row has at least 4 numeric columns.

    Args:
        filepath (str): The path to the CSV file.

    Returns:
        bool: True if the file appears to be a bounding box dataset, False otherwise.
    """
    try:
        with open(filepath, 'r', newline='') as f:
            reader = csv.reader(f)
            first_data_row = next(reader, None)
            if not first_data_row: return False

            # Check for at least 4 columns that can be converted to float.
            if len(first_data_row) >= 4:
                try:
                    float(first_data_row[0]) # xmin
                    float(first_data_row[1]) # ymin
                    float(first_data_row[2]) # xmax
                    float(first_data_row[3]) # ymax
                    return True
                except ValueError:
                    return False # Contains non-numeric values.
            return False # Has fewer than 4 columns.
    except Exception as e:
        logging.warning(f"Error checking bounding box format for {filepath}: {e}")
        return False

# ==============================================================================
# --- FILE AND FOLDER MANAGEMENT HELPERS ---
# ==============================================================================

def create_unique_directory(parent_dir):
    """
    Creates a new directory with a unique name inside a given parent directory.
    The name is based on the current timestamp and a short UUID.

    Args:
        parent_dir (str): The path to the parent directory.

    Returns:
        str: The full path to the newly created unique directory.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = uuid.uuid4().hex[:8]
    dirname = os.path.join(parent_dir, f"collection_{timestamp}_{unique_id}")
    os.makedirs(dirname, exist_ok=True)
    return dirname

def zip_directories(dir_paths, zip_filename, base_path=None):
    """
    Zips one or more directories into a single zip file.

    Args:
        dir_paths (list): A list of directory paths to include in the zip.
        zip_filename (str): The full path of the output zip file.
        base_path (str, optional): A base path to make the paths in the zip relative to.
    """
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for dir_path in dir_paths:
            for root, _, files in os.walk(dir_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, base_path if base_path else dir_path)
                    zipf.write(file_path, arcname)

def wait_for_file_release(path, is_dir=False, timeout=10):
    """
    Waits for a file or directory to be released by another process.
    This is useful on Windows to prevent sharing violation errors.

    Args:
        path (str): The path to the file or directory.
        is_dir (bool): Set to True if the path is a directory.
        timeout (int): The maximum time to wait in seconds.

    Returns:
        bool: True if the resource was released, False on timeout.
    """
    end_time = time.time() + timeout
    while time.time() < end_time:
        try:
            if is_dir:
                os.rename(path, path) # A trick to check if a directory is in use.
            else:
                with open(path, 'a'): pass # Tries to open the file in append mode.
            return True
        except OSError:
            time.sleep(0.1)
    return False

def robust_rmtree(path):
    """
    A more robust version of shutil.rmtree that retries on access errors
    and waits for the directory to be released first.

    Args:
        path (str): The path to the directory tree to remove.

    Returns:
        bool: True on successful removal, False otherwise.
    """
    for attempt in range(3):
        try:
            if not wait_for_file_release(path, is_dir=True):
                logging.error(f"Timeout waiting for release before deleting: {path}")
                return False
            shutil.rmtree(path)
            return True
        except OSError as e:
            if e.winerror == 5 and attempt < 2:  # Access Denied
                logging.warning(f"Access denied to {path}, retrying...")
                time.sleep(1)
            else:
                logging.error(f"Failed to remove {path}: {e}")
                return False
    return False

# ==============================================================================
# --- API ENDPOINTS FOR FOLDER MANAGEMENT (CRUD) ---
# ==============================================================================

@app.route('/api/folders', methods=['POST'])
def create_folder():
    """
    API endpoint to create a new folder within one of the base directories.
    Expects a JSON body with 'newFolderName' and 'baseDir'.
    """
    data = request.json
    new_folder_name = data.get('newFolderName')
    base_dir_key = data.get('baseDir')

    if not new_folder_name or not base_dir_key:
        return jsonify({'error': 'Missing newFolderName or baseDir'}), 400
    
    base_path = PARENT_DIRS.get(base_dir_key)
    if not base_path:
        return jsonify({'error': f'Invalid base directory key: {base_dir_key}'}), 400

    new_path = os.path.join(base_path, new_folder_name)
    try:
        os.makedirs(new_path, exist_ok=False)
        return jsonify({'message': f'Folder "{new_folder_name}" created successfully.'}), 201
    except FileExistsError:
        return jsonify({'error': f'Folder "{new_folder_name}" already exists.'}), 409
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/folders/<path:folder_name>', methods=['DELETE'])
def delete_folder(folder_name):
    """
    API endpoint to delete a folder or file.
    Expects 'base_dir' as a query parameter.
    """
    base_dir_key = request.args.get('base_dir')
    if not base_dir_key:
        return jsonify({'error': 'Missing base_dir query parameter'}), 400

    base_path = PARENT_DIRS.get(base_dir_key)
    if not base_path:
        return jsonify({'error': f'Invalid base directory key: {base_dir_key}'}), 400

    target_path = os.path.join(base_path, folder_name)
    logging.debug(f"Delete request for: {target_path}")

    try:
        if not os.path.exists(target_path):
            return jsonify({'error': f'Target "{folder_name}" not found.'}), 404

        if os.path.isdir(target_path):
            if not robust_rmtree(target_path):
                raise OSError("Failed to remove directory.")
        else: # It's a file
            os.remove(target_path)
        
        return jsonify({'message': f'"{folder_name}" deleted successfully.'}), 200
    except Exception as e:
        logging.error(f"Error deleting {target_path}: {e}")
        return jsonify({'error': f'Error deleting "{folder_name}": {str(e)}'}), 500

@app.route('/api/folders/<path:old_name>', methods=['PUT'])
def rename_folder(old_name):
    """
    API endpoint to rename a folder or file.
    Expects a JSON body with 'newName' and 'baseDir'.
    """
    data = request.json
    new_name = data.get('newName')
    base_dir_key = data.get('baseDir')

    if not new_name or not base_dir_key:
        return jsonify({'error': 'Missing newName or baseDir'}), 400

    base_path = PARENT_DIRS.get(base_dir_key)
    if not base_path:
        return jsonify({'error': f'Invalid base directory key'}), 400

    old_path = os.path.join(base_path, old_name)
    # Construct the new path by replacing the basename of the old path.
    new_path = os.path.join(os.path.dirname(old_path), new_name)

    try:
        if not os.path.exists(old_path):
            return jsonify({'error': f'"{old_name}" not found.'}), 404
        if os.path.exists(new_path):
            return jsonify({'error': f'"{new_name}" already exists.'}), 409

        os.rename(old_path, new_path)
        return jsonify({'message': f'Renamed successfully to "{new_name}".'}), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/dataset/rename', methods=['POST'])
def rename_dataset_globally():
    """
    Handles a "deep rename" for a dataset. It finds all files and folders
    across multiple key directories that contain the old dataset ID and renames
    them to use the new ID. It includes a check to prevent name collisions.
    """
    data = request.json
    old_id = data.get('old_id')
    new_id = data.get('new_id')

    if not old_id or not new_id:
        return jsonify({'error': 'Missing old_id or new_id in request.'}), 400

    # Define the directories to scan for dataset-related files and folders.
    relevant_dir_keys = [
        'parent_dir_dataset', 'indexes', 'range_query_results', 'trainingSets',
        'datasetsAugmentation', 'parent_dir_input_ds', 'parent_dir_rank',
        'fractalDimension', 'parent_dir_histogram'
    ]
    dirs_to_scan = [PARENT_DIRS[key] for key in relevant_dir_keys if key in PARENT_DIRS]

    # --- Step 1: Check for uniqueness of the new_id to prevent collisions ---
    for dir_path in dirs_to_scan:
        if not os.path.isdir(dir_path): continue
        for item_name in os.listdir(dir_path):
            if new_id in item_name:
                logging.warning(f"Rename blocked: new_id '{new_id}' already exists in item '{item_name}' in {dir_path}")
                return jsonify({'error': f"The name '{new_id}' is already in use by another dataset. Please choose a different name."}), 409 # 409 Conflict

    # --- Step 2: Find all items with the old_id and prepare for rename ---
    items_to_rename = []
    for dir_path in dirs_to_scan:
        if not os.path.isdir(dir_path): continue
        for item_name in os.listdir(dir_path):
            if old_id in item_name:
                old_path = os.path.join(dir_path, item_name)
                new_name = item_name.replace(old_id, new_id)
                new_path = os.path.join(dir_path, new_name)
                items_to_rename.append((old_path, new_path))
    
    if not items_to_rename:
        return jsonify({'error': f"No files or folders found for dataset ID '{old_id}'."}), 404

    # --- Step 3: Perform the rename operations ---
    renamed_count = 0
    errors = []
    for old_path, new_path in items_to_rename:
        try:
            os.rename(old_path, new_path)
            renamed_count += 1
            logging.info(f"Renamed: {old_path} -> {new_path}")
        except OSError as e:
            logging.error(f"Failed to rename {old_path} to {new_path}: {e}")
            errors.append(f"Could not rename {os.path.basename(old_path)}.")
    
    if errors:
        return jsonify({
            'error': 'One or more items could not be renamed.',
            'details': errors,
            'message': f"Successfully renamed {renamed_count} out of {len(items_to_rename)} items."
        }), 500

    return jsonify({'message': f"Successfully renamed {renamed_count} items for dataset '{new_id}'."}), 200

@app.route('/zip/folder/<path:folder_name>', methods=['GET'])
def zip_folder(folder_name):
    """
    API endpoint to zip a folder. Returns the name of the created zip file.
    Expects 'base_dir' as a query parameter.
    """
    base_dir_key = request.args.get('base_dir')
    if not base_dir_key:
        return jsonify({'error': 'Missing base_dir query parameter'}), 400

    base_path = PARENT_DIRS.get(base_dir_key)
    if not base_path:
        return jsonify({'error': f'Invalid base directory key'}), 400

    folder_path = os.path.join(base_path, folder_name)
    sanitized_folder_name = os.path.basename(folder_name).replace(os.sep, '_')
    zip_filename_basename = f"{sanitized_folder_name}.zip"
    zip_filename_full_path = os.path.join(base_path, zip_filename_basename)

    try:
        if not os.path.isdir(folder_path):
            return jsonify({'error': f'Folder "{folder_name}" not found.'}), 404

        zip_directories([folder_path], zip_filename_full_path, base_path)
        
        return jsonify({'zip_filename': zip_filename_basename}), 200
    except Exception as e:
        logging.error(f"Error zipping folder: {e}")
        return jsonify({'error': str(e)}), 500

# ==============================================================================
# --- API ENDPOINTS FOR generator.py SCRIPT ---
# ==============================================================================

@socketio.on('generate_data')
def generate_data(data):
    """
    Handles the 'generate_data' event to create datasets based on manual input from the frontend.
    It creates a unique directory for the output, saves the input parameters to a summary CSV,
    and then runs 'generator.py' as a subprocess for each dataset configuration.

    Args:
        data (dict): A dictionary from the frontend containing:
                     - datasets: A list of dictionaries, each with parameters for one dataset.
                     - folder (optional): A subfolder within 'parent_dir_dataset' to save the results.
    """
    # Create a threading event to signal the resource monitoring background task to stop.
    stop_monitoring_event = threading.Event()
    
    try:
        # --- 1. Directory and File Path Setup ---
        # Determine the base directory for saving generated datasets.
        base_path_for_generation = PARENT_DIRS['parent_dir_dataset']
        target_folder = data.get('folder')
        parent_for_new_unique_dir = os.path.join(base_path_for_generation, target_folder) if target_folder else base_path_for_generation
        os.makedirs(parent_for_new_unique_dir, exist_ok=True)
        
        # Create a new, unique directory for this generation session to prevent file collisions.
        main_dataset_dir = create_unique_directory(parent_for_new_unique_dir)
        generated_folder_basename = os.path.basename(main_dataset_dir)

        # Prepare path for a summary CSV file that will store the input parameters for this session.
        # This creates a reproducible record of the generation task.
        input_params_dir = PARENT_DIRS['parent_dir_input_ds']
        os.makedirs(input_params_dir, exist_ok=True)
        input_csv_filename = f"input_params_{generated_folder_basename}.csv"
        input_csv_path = os.path.join(input_params_dir, input_csv_filename)

        # --- 2. Prepare Input Data and Write Summary CSV ---
        datasets_to_process = data.get('datasets', [])
        total_datasets = len(datasets_to_process)
        
        # Sequentially name each dataset and add this name to its parameter dictionary.
        for i, dataset_params in enumerate(datasets_to_process):
            file_format = dataset_params.get('format', 'csv').lower()
            dataset_params['generated_filename'] = f"dataset{i + 1}.{file_format}"
        
        # If there are datasets to process, write their parameters to the summary CSV file.
        if datasets_to_process:
            csv_columns_order = ['datasetName', 'distribution', 'geometry', 'x1', 'y1', 'x2', 'y2','num_features', 'max_seg', 'num_points', 'avg_area','avg_side_length_0', 'avg_side_length_1', 'E0', 'E2']
            with open(input_csv_path, 'w', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=csv_columns_order, delimiter=';')
                writer.writeheader()
                for params in datasets_to_process:
                    # Map parameters from the frontend to the columns of the summary CSV.
                    row_data = {'datasetName': os.path.splitext(params.get('generated_filename', ''))[0],'distribution': params.get('distribution', ''),'geometry': params.get('geometry', ''),'x1': params.get('x1', ''), 'y1': params.get('y1', ''),'x2': params.get('x2', ''), 'y2': params.get('y2', ''),'num_features': params.get('cardinality', ''),'max_seg': params.get('maxseg', ''),'avg_area': params.get('polysize', '')}
                    # Handle composite values like 'maxsize', splitting it into two separate columns.
                    maxsize_values = params.get('maxsize')
                    if isinstance(maxsize_values, str):
                        parts = maxsize_values.split(',')
                        row_data['avg_side_length_0'] = parts[0] if len(parts) > 0 else ''
                        row_data['avg_side_length_1'] = parts[1] if len(parts) > 1 else ''
                    writer.writerow(row_data)

        # --- 3. Execute Dataset Generation ---
        # Start a background task to monitor system resources (CPU/memory) for the duration of the generation.
        socketio.start_background_task(monitor_and_emit_usage, stop_monitoring_event)

        # Iterate through each dataset configuration and run the generator script for it.
        for i, dataset in enumerate(datasets_to_process):
            output_path = os.path.join(main_dataset_dir, dataset['generated_filename'])
            
            # Dynamically build the command-line arguments for 'generator.py' from the dataset parameters.
            cmd = ["python", "generator.py"]
            for key, value in dataset.items():
                # Append only valid parameters, skipping internal ones like 'id'. `shlex.quote` prevents command injection.
                if value is not None and str(value).strip() != '' and key not in ['id', 'generated_filename']:
                    cmd.append(f"{key}={shlex.quote(str(value))}")

            logging.debug(f'Executing command: {cmd}')
            # Execute the script as a subprocess, capturing stdout and stderr.
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')
            
            # Stream the generator's output (stdout) directly to the destination file.
            # This is memory-efficient as it avoids loading the entire dataset into memory.
            with open(output_path, 'w', encoding='utf-8') as f:
                for line in iter(process.stdout.readline, ''):
                    f.write(line)
                    socketio.sleep(0) # Yield control to other concurrent tasks (like monitoring).

            # Wait for the process to complete and capture any error output.
            _, stderr = process.communicate()

            # If the script fails, emit an error to the client and stop the entire process.
            if process.returncode != 0:
                error_message = stderr.strip() if stderr else "Unknown error"
                emit('generate_data_error', {'error': f'Error for dataset {i + 1}: {error_message}'})
                return
            
            # Update the client with the progress of the generation task.
            progress = int(((i + 1) / total_datasets) * 100)
            emit('progress', {'progress': progress})

        # --- 4. Finalize ---
        # Once all datasets are generated, notify the client of completion.
        emit('generate_data_complete', {'dataset_id': generated_folder_basename, 'input_csv_file': input_csv_filename})

    except Exception as e:
        # Catch any unexpected server-side exceptions and report them to the client.
        logging.exception('Exception during manual data generation.')
        emit('generate_data_error', {'error': str(e)})
    finally:
        # Ensure the resource monitoring background task is always stopped.
        stop_monitoring_event.set()

@socketio.on('generate_data_from_csv')
def generate_data_from_csv(data):
    logging.debug('Received generate_data_from_csv event')
    stop_monitoring_event = threading.Event()
    
    try:
        # --- 1. Directory Setup and CSV Decoding ---
        base_path_for_generation = PARENT_DIRS['parent_dir_dataset']
        target_folder = data.get('folder')
        parent_for_new_unique_dir = os.path.join(base_path_for_generation, target_folder) if target_folder else base_path_for_generation
        os.makedirs(parent_for_new_unique_dir, exist_ok=True)
        main_dataset_dir = create_unique_directory(parent_for_new_unique_dir)
        generated_folder_basename = os.path.basename(main_dataset_dir)

        input_csv_filename = f"input_params_{generated_folder_basename}.csv"
        input_csv_path = os.path.join(PARENT_DIRS['parent_dir_input_ds'], input_csv_filename)
        csv_file_content = data['csvFile'].split(',')[1]
        csv_file_bytes = base64.b64decode(csv_file_content)
        
        try:
            csv_file_string = csv_file_bytes.decode('utf-8')
        except UnicodeDecodeError:
            csv_file_string = csv_file_bytes.decode('latin-1') # Fallback
            
        # --- 2. Parse Uploaded CSV and Prepare Data with Header Normalization ---
        uploaded_csv_data = []
        delimiter = ';'
        
        csv_file_string = csv_file_string.lstrip('\ufeff')

        # 1. Read header line and clean field names to handle whitespace/BOM issues
        csv_input_for_header = io.StringIO(csv_file_string)
        header_line = csv_input_for_header.readline().strip()
        
        if not header_line:
            emit('generate_data_error', {'error': 'Uploaded CSV is empty or has no header.'})
            stop_monitoring_event.set()
            return

        # Clean all field names (strip whitespace)
        clean_header_fields = [field.strip() for field in header_line.split(delimiter)]
        clean_header_line = delimiter.join(clean_header_fields)
        
        # 2. Reconstruct the clean content string
        # Prepend the clean header and append the original data lines (skipping the original header)
        content_lines = csv_file_string.splitlines()
        
        if len(content_lines) > 1:
            clean_csv_string = clean_header_line + '\n' + '\n'.join(content_lines[1:])
        else:
            clean_csv_string = clean_header_line

        # 3. Use DictReader with the clean content
        csv_input_for_read = io.StringIO(clean_csv_string)
        reader = csv.DictReader(csv_input_for_read, delimiter=delimiter) 

        # Check if the 'datasetName' column exists (now checked against the clean names)
        fieldnames = reader.fieldnames
        if not fieldnames or 'datasetName' not in fieldnames:
            # Fallback: se fallisce, stampa i nomi delle colonne per debug
            logging.error(f"Failed to find 'datasetName'. Found fields: {fieldnames}")
            emit('generate_data_error', {'error': f'Uploaded CSV must contain a "datasetName" column. Found columns: {fieldnames}'})
            stop_monitoring_event.set()
            return
            
        for i, row in enumerate(reader):
            # Check if the datasetName is present in the row
            if not row.get('datasetName'):
                emit('generate_data_error', {'error': f'Row {i+2} in CSV is missing a value for "datasetName".'})
                stop_monitoring_event.set()
                return
            
            uploaded_csv_data.append(row)

        # Write the parsed (and slightly modified) data to a new server-side summary CSV file.
        csv_columns_order = ['datasetName', 'distribution', 'geometry', 'x1', 'y1', 'x2', 'y2','num_features', 'max_seg', 'num_points', 'avg_area','avg_side_length_0', 'avg_side_length_1', 'E0', 'E2']
        with open(input_csv_path, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_columns_order, delimiter=';')
            writer.writeheader()
            for row_from_uploaded in uploaded_csv_data:
                # Map columns from the uploaded CSV to the standard server-side format.
                writer.writerow({'datasetName': row_from_uploaded.get('datasetName', ''),'distribution': row_from_uploaded.get('distribution', ''),'geometry': row_from_uploaded.get('geometry', ''),'x1': row_from_uploaded.get('x1', ''), 'y1': row_from_uploaded.get('y1', ''),'x2': row_from_uploaded.get('x2', ''), 'y2': row_from_uploaded.get('y2', ''),'num_features': row_from_uploaded.get('num_features', ''),'max_seg': row_from_uploaded.get('max_seg', ''),'avg_area': row_from_uploaded.get('avg_area', ''),'avg_side_length_0': row_from_uploaded.get('avg_side_length_0', ''),'avg_side_length_1': row_from_uploaded.get('avg_side_length_1', ''),})
        
        # --- 3. Build Generator Commands from CSV Data ---
        commands = []
        output_filenames = []
        for i, row in enumerate(uploaded_csv_data):
            # Determine file extension based on geometry type.
            geometry = row.get('geometry', '').lower()
            file_extension = 'wkt' if geometry == 'polygon' else 'csv'

            dataset_name_from_csv = row.get('datasetName')
            output_filename = os.path.join(main_dataset_dir, f"{dataset_name_from_csv}.{file_extension}")
            output_filenames.append(output_filename)
            
            # Calculate the affine transformation matrix from coordinates.
            try:
                x1, y1, x2, y2 = float(row.get('x1',0)), float(row.get('y1',0)), float(row.get('x2',0)), float(row.get('y2',0))
                affinematrix = f"{x2-x1},0,{x1},0,{y2-y1},{y1}"
            except (ValueError, TypeError):
                emit('generate_data_error', {'error': 'Invalid x1, y1, x2, y2 values in CSV.'})
                return
            
            # Dynamically construct the command-line arguments for 'generator.py'.
            cmd = ["python", "generator.py", f"affinematrix={shlex.quote(affinematrix)}"]
            # Use a map to translate CSV column names to the script's expected argument names.
            param_map = {'distribution': 'distribution', 'num_features': 'cardinality','avg_area': 'polysize', 'geometry': 'geometry', 'max_seg': 'maxseg'}
            for csv_col, script_arg in param_map.items():
                if row.get(csv_col): cmd.append(f"{script_arg}={shlex.quote(row[csv_col])}")
            
            # Add fixed or conditional arguments based on the data in the CSV row.
            cmd.append("dimensions=2")
            cmd.append(f"format={file_extension}")
            if row.get('distribution') != "parcel" and row.get('geometry') == "box":
                cmd.append(f"maxsize={shlex.quote(row.get('avg_side_length_0',''))},{shlex.quote(row.get('avg_side_length_1',''))}")
            
            # Add distribution-specific parameters.
            distribution = row.get('distribution', '').lower()
            if distribution == 'diagonal':
                cmd.extend(["percentage=0.5", "buffer=0.5"])
            elif distribution == 'bit':
                cmd.extend(["probability=0.2", "digits=10"])
            elif distribution == 'parcel':
                cmd.extend(["srange=0.5", "dither=0.5"])
            commands.append(cmd)

        # --- 4. Execute Generation and Finalize ---
        socketio.start_background_task(monitor_and_emit_usage, stop_monitoring_event)

        progress_per_dataset = 100 / len(commands) if commands else 100
        last_emitted_progress = -1

        for i, cmd in enumerate(commands):
            logging.debug(f'Executing command: {cmd}')
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')

            try:
                total_lines = int(uploaded_csv_data[i].get('num_features', 0))
            except (ValueError, TypeError):
                total_lines = 0

            lines_written = 0
            base_progress = i * progress_per_dataset

            # Stream output directly to the corresponding file.
            with open(output_filenames[i], 'w', encoding='utf-8') as f:
                for line in iter(process.stdout.readline, ''):
                    f.write(line)
                    lines_written += 1

            if total_lines > 0:
                sub_progress = (lines_written / total_lines) * progress_per_dataset
                current_total_progress = int(base_progress + sub_progress)

                if current_total_progress > last_emitted_progress:
                    emit('progress', {'progress': current_total_progress})
                    last_emitted_progress = current_total_progress
            
            socketio.sleep(0)
            
            _, stderr = process.communicate()

            if process.returncode != 0:
                emit('generate_data_error', {'error': f'Generator script failed: {stderr.strip() if stderr else "Unknown error"}'})
                return

            final_progress_step = int((i + 1) * progress_per_dataset)
            if final_progress_step > last_emitted_progress:
                emit('progress', {'progress': final_progress_step})
                last_emitted_progress = final_progress_step

        emit('generate_data_complete', {'dataset_id': generated_folder_basename, 'input_csv_file': input_csv_filename})

    except Exception as e:
        # Catch any unexpected server-side exceptions.
        logging.exception('Exception during CSV data generation.')
        emit('generate_data_error', {'error': str(e)})
    finally:
        # Ensure the resource monitoring is always stopped.
        stop_monitoring_event.set()

# ==============================================================================
# --- API ENDPOINTS FOR DOWNLOADING & FILE PREVIEW ---
# ==============================================================================

@app.route('/download/<path:item_name>')
def download_item(item_name):
    """
    API endpoint to download a file. It can be a single file or a zip archive.
    Expects 'base_dir' as a query parameter.
    """
    base_dir_key = request.args.get('base_dir')
    if not base_dir_key: return jsonify({'error': 'Missing base_dir query parameter'}), 400

    base_path = PARENT_DIRS.get(base_dir_key)
    if not base_path: return jsonify({'error': f'Invalid base directory key: {base_dir_key}'}), 400

    file_path = os.path.join(base_path, item_name)
    try:
        if not os.path.exists(file_path):
            return jsonify({'error': f'File "{item_name}" not found.'}), 404
        return send_from_directory(base_path, item_name, as_attachment=True)
    except Exception as e:
        logging.error(f"Error during file download: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/download/confirm/<filename>', methods=['POST'])
def confirm_download(filename):
    """
    API endpoint to delete a temporary zip file after the client has confirmed download.
    Expects 'base_dir' as a query parameter.
    """
    base_dir_key = request.args.get('base_dir')
    if not base_dir_key: return jsonify({'error': 'Missing base_dir query parameter'}), 400
    
    base_path = PARENT_DIRS.get(base_dir_key)
    if not base_path: return jsonify({'error': f'Invalid base directory key'}), 400

    file_path = os.path.join(base_path, filename)
    try:
        if os.path.exists(file_path) and filename.endswith('.zip'):
            if wait_for_file_release(file_path):
                os.remove(file_path)
                return jsonify({'message': 'Temporary zip file deleted.'}), 200
            else:
                return jsonify({'error': 'Timeout waiting for file release.'}), 500
        return jsonify({'message': 'File not found or not a zip file.'}), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/preview/file/<path:item_name>', methods=['GET'])
def preview_file_content(item_name):
    """
    API endpoint to get the content of a file for previewing in the frontend.
    Returns the content as plain text. Expects 'base_dir' as a query parameter.
    """
    base_dir_key = request.args.get('base_dir')
    if not base_dir_key: return jsonify({'error': 'Missing base_dir query parameter'}), 400

    base_path = PARENT_DIRS.get(base_dir_key)
    if not base_path: return jsonify({'error': f'Invalid base directory key'}), 400

    file_path = os.path.abspath(os.path.join(base_path, item_name))
    # Security check to prevent directory traversal.
    if not file_path.startswith(os.path.abspath(base_path)):
        return jsonify({'error': 'Directory traversal attempt detected.'}), 403

    try:
        if not os.path.exists(file_path) or os.path.isdir(file_path):
            return jsonify({'error': 'File not found or is a directory.'}), 404

        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        response = make_response(content)
        response.headers['Content-Type'] = 'text/plain; charset=utf-8'
        return response
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ==============================================================================
# --- API ENDPOINTS FOR NEW FileExplorer COMPONENT ---
# ==============================================================================

# Maps internal directory keys to user-friendly labels for the UI.
ROOT_DIR_LABELS = {
    'parent_dir_dataset': '1. Generator', 'indexes': '2. Index',
    'range_query_results': '3. Range Query', 'trainingSets': '4. Training Sets',
    'datasetsAugmentation': '5. Augmented Datasets', 'augmentation_logs' : '5.5 Augmentation Logs',
    'parent_dir_histogram': '6. Histogram', 'fractalDimension': '#. Fractal Dimension','parent_dir_input_ds': 'a. Input Collection Files',
    'parent_dir_rq_input': 'b. Input Range Query Files', 'parent_dir_rank': 'c. Input Balacing Analysis',
}


@app.route('/api/explorer/roots', methods=['GET'])
def get_explorer_roots():
    """
    API endpoint to provide the initial root-level folders for the FileExplorer component.
    The order is predefined for a consistent user experience.
    """
    # Define the exact order for output and input folders.
    output_folders_order = ['parent_dir_dataset', 'indexes', 'range_query_results', 'trainingSets', 'datasetsAugmentation', 'augmentation_logs', 'parent_dir_histogram', 'fractalDimension']
    input_folders = ['parent_dir_input_ds', 'parent_dir_rq_input', 'parent_dir_rank']

    root_nodes = []
    # Add output folders.
    for key in output_folders_order:
        if key in PARENT_DIRS and key in ROOT_DIR_LABELS:
            root_nodes.append({'key': key, 'label': ROOT_DIR_LABELS[key], 'data': {'path': key}, 'type': 'folder', 'leaf': False})
    
    # Add a visual separator.
    root_nodes.append({'key': 'separator', 'type': 'separator', 'selectable': False})

    # Add input folders.
    for key in input_folders:
        if key in PARENT_DIRS and key in ROOT_DIR_LABELS:
            root_nodes.append({'key': key, 'label': ROOT_DIR_LABELS[key], 'data': {'path': key}, 'type': 'folder', 'leaf': False})

    return jsonify(root_nodes)


@app.route('/api/explorer/content', methods=['GET'])
def get_explorer_content():
    """
    API endpoint to get the content of a specific directory for lazy loading in the FileExplorer.
    Expects a 'path' query parameter (e.g., 'parent_dir_dataset/my_folder').
    """
    relative_path = request.args.get('path')
    if not relative_path: return jsonify({'error': 'Missing path parameter'}), 400

    path_parts = relative_path.replace('\\', '/').split('/')
    root_key = path_parts[0]

    if root_key not in PARENT_DIRS:
        return jsonify({'error': f'Invalid root path key: {root_key}'}), 400

    # Reconstruct and validate the full path to prevent security issues.
    base_path = PARENT_DIRS[root_key]
    sub_path = os.path.join(*path_parts[1:]) if len(path_parts) > 1 else ''
    full_path = os.path.abspath(os.path.join(base_path, sub_path))

    if not full_path.startswith(os.path.abspath(base_path)):
         return jsonify({'error': 'Directory traversal attempt detected'}), 403
    if not os.path.isdir(full_path):
        return jsonify({'error': 'Path not found or is not a directory'}), 404

    # Read directory content and format it for the PrimeReact Tree component.
    try:
        content = []
        for item_name in os.listdir(full_path):
            # Skip .crc files if in Windows ambient
            if item_name.endswith('.crc'):
                continue
            item_full_path = os.path.join(full_path, item_name) # Path for os checks
            item_relative_path = os.path.join(relative_path, item_name).replace('\\', '/')
            if os.path.isdir(item_full_path):
                if root_key == 'trainingSets' and re.match(r'^training_set_\d+_diff$', item_name):
                    continue
                content.append({'key': item_relative_path, 'label': item_name, 'data': {'path': item_relative_path}, 'type': 'folder', 'leaf': False})
            else:
                # Get the size of the file in bytes.
                file_size = os.path.getsize(item_full_path)
                content.append({'key': item_relative_path, 'label': item_name, 'data': {'path': item_relative_path}, 'type': 'file', 'leaf': True, 'size': file_size})
        
        # Sort content alphabetically, with folders appearing before files.
        content.sort(key=lambda x: (x['type'] != 'folder', x['label'].lower()))
        return jsonify(content)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ==============================================================================
# --- API ENDPOINTS FOR Augmentation SCRIPT ---
# ==============================================================================

@app.route('/api/augmentation/distributions', methods=['GET'])
def get_distributions_for_training_set():
    """
    Finds the summary file corresponding to a given bin file and returns the unique
    values from its 'distribution' column. This populates the distribution dropdown on the frontend.
    """
    bin_file_relative_path = request.args.get('bin_file_path')
    if not bin_file_relative_path: 
        return jsonify({"error": "Missing 'bin_file_path' parameter."}), 400

    try:
        # Get path and root
        path_parts = bin_file_relative_path.replace('\\', '/').split('/')
        root_key = path_parts[0]
        
        if root_key not in PARENT_DIRS:
            return jsonify({"error": f"Invalid root key in path: {root_key}"}), 400

        # Build path
        base_path = os.path.abspath(PARENT_DIRS[root_key])
        sub_path = os.path.join(*path_parts[1:]) if len(path_parts) > 1 else ''
        full_bin_path = os.path.abspath(os.path.join(base_path, sub_path))

        # Check on path correctness
        if not full_bin_path.startswith(base_path): 
            return jsonify({"error": "Directory traversal attempt."}), 403
        
        bin_dir = os.path.dirname(full_bin_path)
        
        # Extract unique code from bin file
        bin_filename = os.path.basename(full_bin_path)
        unique_code_part = bin_filename.replace('bin_', '', 1) 

        summary_filename = f'input_params_{unique_code_part}'
        summary_full_path = os.path.join(bin_dir, summary_filename)
        
        if not os.path.exists(summary_full_path): 
            return jsonify({"error": f"Corresponding summary file not found at {summary_full_path}"}), 404

        # Read the CSV with pandas and extract unique distributions.
        df = pd.read_csv(summary_full_path, delimiter=';')
        if 'distribution' not in df.columns: 
            return jsonify({"error": "'distribution' column not found."}), 500
        
        unique_distributions = df['distribution'].dropna().unique().tolist()
        return jsonify(unique_distributions)
    except Exception as e:
        return jsonify({"error": f"An unexpected error occurred: {str(e)}"}), 500

@app.route('/api/augmentation/bin-file-content', methods=['GET'])
def get_bin_file_content():
    """
    Returns the content of a specified bin file as JSON, formatted for a DataTable.
    """
    file_path_relative = request.args.get('path')
    if not file_path_relative: return jsonify({"error": "Missing 'path' parameter."}), 400

    try:
        # Get path and root
        path_parts = file_path_relative.replace('\\', '/').split('/')
        root_key = path_parts[0]
        
        if root_key not in PARENT_DIRS:
            return jsonify({"error": f"Invalid root key in path: {root_key}"}), 400

        # Create the full path
        base_path = os.path.abspath(PARENT_DIRS[root_key])
        sub_path = os.path.join(*path_parts[1:]) if len(path_parts) > 1 else ''
        full_path = os.path.abspath(os.path.join(base_path, sub_path))
        
        # 3. Check on path correctness
        if not full_path.startswith(base_path): 
            return jsonify({"error": "Directory traversal attempt."}), 403
        
        if not os.path.exists(full_path): 
            return jsonify({"error": "Bin file not found."}), 404

        # Read with pandas, rename the first column to 'bin' for consistency, and return as JSON.
        df = pd.read_csv(full_path, delimiter=';')
        if not df.empty:
            df.rename(columns={df.columns[0]: 'bin'}, inplace=True)
        df.columns = [str(col).lower().strip() for col in df.columns] # Standardize column names.
        return jsonify(df.to_dict(orient='records'))
    except Exception as e:
        return jsonify({"error": f"Error reading bin file: {str(e)}"}), 500

@socketio.on('run_augmentation')
def run_augmentation_socket(data):
    """
    Handles the 'run_augmentation' event to execute the main augmentation script.
    It constructs the necessary configuration files, runs 'augmentation.py' as a subprocess,
    and streams progress by parsing the script's stdout while also saving the full output to a log file.
    """
    logging.debug(f"Received 'run_augmentation' event with data: {data}")
    stop_monitoring_event = threading.Event()

    try:
        # Validate incoming data from the frontend
        if not all(field in data for field in ['trainingSetPath', 'pathDatasets', 'pathIndexes', 'inputs']):
            emit('augmentation_error', {"error": "Missing required fields."})
            return

        # Securely construct the absolute path to the training set directory
        training_set_dir_relative = data['trainingSetPath']
        path_parts = training_set_dir_relative.replace('\\', '/').split('/')
        root_key = path_parts[0]  # This should be 'trainingSets'.

        if root_key not in PARENT_DIRS:
            emit('augmentation_error', {"error": f"Invalid root key for training set path: {root_key}"})
            return

        # Safely join the secure base path (e.g., "/data/trainingSets") with the rest of the relative path.
        base_path = os.path.abspath(PARENT_DIRS[root_key])
        sub_path = os.path.join(*path_parts[1:])
        full_ts_path = os.path.abspath(os.path.join(base_path, sub_path))

        # Ensure the target directory exists before trying to write to it.
        os.makedirs(full_ts_path, exist_ok=True)

        # Create the 'input.csv' file for the augmentation script
        input_file_path = os.path.join(full_ts_path, 'input.csv')
        with open(input_file_path, 'w', newline='', encoding='utf-8') as f:
            for task in data['inputs']:
                # The script expects 'bin' numbers in 'binX' format (e.g., 'bin4').
                bin_num_str = str(task.get('bin_num', '')).lower().replace(" ", "")
                f.write(
                    f"{bin_num_str} {task.get('num_queries', '')} {task.get('distribution', '')} {' '.join(task.get('augmentation_technique', []))}\n")

        # Create the 'augmentationParameters.csv' configuration file
        params_file_path = os.path.join(DATA_BASE_PATH, 'augmentationParameters.csv')
        with open(params_file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, delimiter=';')
            writer.writerow(['pathTrainingSet', 'nameBin', 'nameSummary', 'nameRangeQueriesResult', 'nameInputs',
                             'pathDatasets', 'pathIndexes'])

            # Securely construct the absolute paths for datasets and indexes.
            path_datasets_full = os.path.abspath(os.path.join(PARENT_DIRS['parent_dir_dataset'], data['pathDatasets'].split('/')[1]))
            path_indexes_full = os.path.abspath(os.path.join(PARENT_DIRS['indexes'], data['pathIndexes'].split('/')[1]))

            # This logic correctly derives the session folder name (e.g., "dataset_2025...")
            # from the full path of the specific training set folder (e.g., ".../training_set_1").
            ts_folder_name = os.path.basename(os.path.dirname(full_ts_path))
            
            bin_file_name = f'bin_{ts_folder_name}_ts.csv'
            summary_file_name = f'input_params_{ts_folder_name}_ts.csv'
            rq_result_file_name = f'rqR_{ts_folder_name}_ts.csv'

            writer.writerow([training_set_dir_relative, bin_file_name, summary_file_name, rq_result_file_name, 'input.csv',
                             path_datasets_full, path_indexes_full])

        # Execute the augmentation script as a subprocess
        log_filename = f"aug_{ts_folder_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        log_filepath = os.path.join(PARENT_DIRS['augmentation_logs'], log_filename)

        script_dir = os.path.dirname(os.path.abspath(__file__))
        augmentation_script_path = os.path.join(script_dir, 'augmentation.py')
        
        # Use '-u' for unbuffered output, allowing real-time parsing of stdout for progress updates.
        cmd = ['python', '-u', augmentation_script_path]
        total_tasks = len(data.get('inputs', []))
        
        # Open the log file and start the subprocess.
        with open(log_filepath, 'w', encoding='utf-8') as log_file:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True,
                                       encoding='utf-8', bufsize=1, cwd=DATA_BASE_PATH)

            # Start the process-specific monitoring.
            socketio.start_background_task(monitor_process_and_emit_usage, process, stop_monitoring_event)

            # Read stdout line by line to parse progress and write to the log.
            for line in process.stdout:
                log_file.write(line) # Write every line to the log file immediately.
                
                # Check for progress indicator strings in the script's output.
                if "PROCESSING INPUT" in line:
                    try:
                        # Extract the task number from the output string.
                        task_index = int(line.split(' ')[2].replace('!', ''))
                        socketio.emit('augmentation_progress',
                                      {"current": task_index, "total": total_tasks,
                                       "message": f"Processing task {task_index} of {total_tasks}..."})
                        socketio.sleep(0) # Yield control to allow the event to be sent.
                    except (ValueError, IndexError):
                        pass

            process.wait() # Wait for the subprocess to complete.

            # Report the final status to the client
            if process.returncode != 0:
                emit('augmentation_error', {"error": "Augmentation script failed.",
                                             "details": f"See log file '{log_filename}' for details."})
            else:
                emit('augmentation_complete', {"message": "Augmentation completed successfully.", "logFile": log_filename})

    except Exception as e:
        logging.exception("Error during augmentation.")
        emit('augmentation_error', {"error": f"An internal server error occurred: {str(e)}"})
    finally:
        # Always ensure the resource monitoring thread is stopped.
        stop_monitoring_event.set()

# ==============================================================================
# --- API ENDPOINTS FOR fractalDimension.py SCRIPT ---
# ==============================================================================

@socketio.on('run_fractal_dimension')
def run_fractal_dimension_socket(data):
    """
    Handles the 'run_fractal_dimension' event from the client to execute the fractal dimension script.
    It dynamically creates the 'fdParameters.csv' configuration file based on user input,
    runs the script as a subprocess, and communicates the results or errors back to the client.

    Args:
        data (dict): A dictionary containing parameters from the frontend:
                     - analysisType: Type of analysis ('distribution', 'summary', 'range_query').
                     - selectedPath: The relative path to the selected file or folder.
                     - parameters: A list of parameters to analyze (e.g., ['cardinality', 'executionTime']).
    """
    logging.debug(f"Received 'run_fractal_dimension' event with data: {data}")
    stop_monitoring_event = threading.Event()

    try:
        # --- 1. Extract and Validate Input ---
        analysis_type = data.get('analysisType')
        selected_path_relative = data.get('selectedPath')
        parameters_list = data.get('parameters', [])
        rq_source = data.get('rqSource')

        if not all([analysis_type, selected_path_relative, parameters_list]):
            emit('fractal_dimension_error', {'error': 'Missing required parameters from the client.'})
            return

        total_datasets_for_progress = 0

        # --- 2. Prepare fdParameters.csv ---
        params_file_path = os.path.join(DATA_BASE_PATH, 'fdParameters.csv')
        
        header_for_script = [
            'pathDatasets', 'pathSummary', 'nameSummary', 'pathRangeQuery_ts', 'nameRangeQuery_ts',
            'fromX', 'toX', 'pathFD', 'pathFD_ts', 'parameters'
        ]
        
        data_row = [''] * 9

        path_parts = selected_path_relative.replace('\\', '/').split('/')
        root_key = path_parts[0]

        if root_key not in PARENT_DIRS:
            emit('fractal_dimension_error', {'error': f"Invalid root key in path: {root_key}"})
            return
            
        base_path_of_selection = os.path.abspath(PARENT_DIRS[root_key])
        sub_path_of_selection = os.path.join(*path_parts[1:]) if len(path_parts) > 1 else ''
        full_selected_path = os.path.abspath(os.path.join(base_path_of_selection, sub_path_of_selection))

        if not full_selected_path.startswith(base_path_of_selection):
            emit('fractal_dimension_error', {'error': 'Directory traversal attempt detected.'})
            return

        if analysis_type == 'distribution':
            folder_name = os.path.basename(full_selected_path)
            summary_file_name = f"input_params_{folder_name}.csv"
            summary_file_path = os.path.join(PARENT_DIRS['parent_dir_input_ds'], summary_file_name)
            
            if not os.path.exists(summary_file_path):
                emit('fractal_dimension_error', {'error': f"Summary file '{summary_file_name}' not found."})
                return
            
            try:
                with open(summary_file_path, 'r', encoding='utf-8') as f:
                    num_datasets = sum(1 for row in csv.reader(f)) - 1
                    if num_datasets < 0: num_datasets = 0
                total_datasets_for_progress = num_datasets
            except Exception as e:
                emit('fractal_dimension_error', {'error': f"Could not read or parse the summary file: {str(e)}"})
                return

            data_row[0] = full_selected_path
            data_row[1] = PARENT_DIRS['parent_dir_input_ds']
            data_row[2] = summary_file_name
            data_row[5] = '1'
            data_row[6] = str(num_datasets)
            
        elif analysis_type == 'summary':
            data_row[1] = os.path.dirname(full_selected_path)
            data_row[2] = os.path.basename(full_selected_path)
            data_row[7] = PARENT_DIRS['fractalDimension']
        
        elif analysis_type == 'range_query':
            output_path_for_fd = ''
            if rq_source == 'training_set':
                output_path_for_fd = os.path.dirname(full_selected_path)
            else:
                output_path_for_fd = PARENT_DIRS['fractalDimension']

            data_row[3] = os.path.dirname(full_selected_path)
            data_row[4] = os.path.basename(full_selected_path)
            data_row[8] = output_path_for_fd

        data_row.extend(parameters_list)

        with open(params_file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f, delimiter=';')
            writer.writerow(header_for_script)
            writer.writerow(data_row)
        
        try:
            with open(params_file_path, 'r', encoding='utf-8') as f:
                csv_content_for_log = f.read().strip()
                logging.info(f"\n--- Content of fdParameters.csv ---\n{csv_content_for_log}\n--------------------------------------")
        except Exception as e:
            logging.error(f"Could not read back fdParameters.csv for debugging: {e}")

        # --- 3. Execute the fractalDimension.py script ---
        script_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "fractalDimension.py")
        cmd = ["python", "-u", script_path]
        
        logging.debug(f"Executing command: {' '.join(cmd)} in CWD: {DATA_BASE_PATH}")
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, cwd=DATA_BASE_PATH, encoding='utf-8', bufsize=1)
        
        socketio.start_background_task(monitor_process_and_emit_usage, process, stop_monitoring_event)
        
        # For single-shot analyses, emit a structured progress event to force the
        # frontend progress bar into a determinate state for a consistent user experience.
        if analysis_type in ['summary', 'range_query']:
            emit('fractal_dimension_dataset_progress', {
                'current': 0, 'total': 1,
                'datasetName': "Analysis in progress..."
            })
            socketio.sleep(0.1)

        current_dataset_count = 0
        output_lines = []

        for line in process.stdout:
            stripped_line = line.strip()
            output_lines.append(line)
            logging.debug(f"Script Output: {stripped_line}")

            if stripped_line.startswith('<System>'):
                emit('fractal_dimension_progress', {'message': stripped_line})
            
            elif stripped_line.startswith('dataset: '):
                try:
                    current_dataset_count += 1
                    file_path = stripped_line.split(':', 1)[1].strip()
                    ds_name = os.path.basename(file_path)
                    emit('fractal_dimension_dataset_progress', {
                        'current': current_dataset_count, 'total': total_datasets_for_progress,
                        'datasetName': ds_name
                    })
                except (ValueError, IndexError):
                    logging.warning(f"Could not parse dataset line: {stripped_line}")
            
            socketio.sleep(0)

        process.wait()
        full_output = "".join(output_lines)

        # For single-shot analyses, emit a final progress update to move the
        # determinate bar to 100% before sending the completion signal.
        if analysis_type in ['summary', 'range_query']:
            emit('fractal_dimension_dataset_progress', {
                'current': 1, 'total': 1,
                'datasetName': "Finalizing results..."
            })
            socketio.sleep(0.1)

        # --- 4. Handle Script Results ---
        if process.returncode != 0:
            logging.error(f"Error executing fractalDimension.py. Full output: {full_output}")
            emit('fractal_dimension_error', {'error': "Script failed. See details below.", 'details': full_output})
        else:
            logging.info(f"fractalDimension.py executed successfully.")
            emit('fractal_dimension_complete', {'message': 'Fractal dimension calculation completed successfully.'})

    except Exception as e:
        logging.exception("Exception during fractal dimension calculation.")
        emit('fractal_dimension_error', {'error': f'An internal server error occurred: {str(e)}'})
    finally:
        stop_monitoring_event.set()


@app.route('/api/fractal/sources', methods=['GET'])
def get_fractal_sources():
    """
    Scans all relevant directories to find unique base dataset IDs.
    Example: 'fd_rqR_dataset_123_ts.csv' -> 'dataset_123'
    """
    all_files = []
    unique_ids = set()

    # Paths to scan for result files
    paths_to_scan = [
        os.path.join(PARENT_DIRS['parent_dir_input_ds'], 'input_params_*.csv'),
        os.path.join(PARENT_DIRS['fractalDimension'], 'fd_*.csv')
    ]

    for path in paths_to_scan:
        all_files.extend(glob.glob(path))

    # Regex to extract the base dataset ID from a filename
    # It captures the 'dataset_YYYYMMDD_HHMMSS_UUID' part
    id_pattern = re.compile(r'(collection_\d{8}_\d{6}_[0-9a-fA-F]{8})')

    for f in all_files:
        match = id_pattern.search(os.path.basename(f))
        if match:
            unique_ids.add(match.group(1))
    
    return jsonify(sorted(list(unique_ids)))


@app.route('/api/fractal/data', methods=['POST'])
def get_fractal_data():
    """
    For each selected dataset ID, this function finds all related Fractal Dimension (FD) 
    result files and structures them into a nested dictionary for detailed visualization.
    The structure separates E2 distribution values, group properties, and range query results
    (differentiating between original and training sets).
    """
    selected_ids = request.json.get('dataset_ids', [])
    if not selected_ids:
        return jsonify({})

    # The main dictionary to hold all results, keyed by dataset_id
    results_by_dataset = {}

    for dataset_id in selected_ids:
        # This dictionary will hold the structured results for the current dataset_id
        current_dataset_results = {
            "e2Distribution": [],
            "groupProperties": [],
            "rangeQueryResults": {
                "original": [],
                "trainingSets": []
            }
        }
        
        # --- Step 1: Fetch E2 Distribution (per-dataset values) ---
        try:
            summary_path = os.path.join(PARENT_DIRS['parent_dir_input_ds'], f"input_params_{dataset_id}.csv")
            if os.path.exists(summary_path):
                df = pd.read_csv(summary_path, delimiter=';')
                # Ensure required columns exist before proceeding
                if 'datasetName' in df.columns and 'E2' in df.columns:
                    # Filter out rows where E2 is NaN or empty
                    e2_data = df[['datasetName', 'E2']].dropna(subset=['E2'])
                    for index, row in e2_data.iterrows():
                        current_dataset_results["e2Distribution"].append({
                            'datasetName': row['datasetName'],
                            'e2Value': float(row['E2'])
                        })
        except Exception as e:
            logging.error(f"Error processing E2 distribution for {dataset_id}: {e}")

        # --- Step 2: Fetch Dataset Group Properties (FD of summary file columns) ---
        try:
            fd_summary_path = os.path.join(PARENT_DIRS['fractalDimension'], f"fd_input_params_{dataset_id}.csv")
            if os.path.exists(fd_summary_path):
                df = pd.read_csv(fd_summary_path, delimiter=';')
                if not df.empty:
                    for param, value in df.iloc[0].dropna().to_dict().items():
                        current_dataset_results["groupProperties"].append({'parameter': param, 'value': float(value)})
        except Exception as e:
            logging.error(f"Error processing Group Properties for {dataset_id}: {e}")

        # --- Step 3: Fetch Range Query Results ---
        try:
            # Original Range Query FD
            fd_rq_path = os.path.join(PARENT_DIRS['fractalDimension'], f"fd_rqR_{dataset_id}.csv")
            if os.path.exists(fd_rq_path):
                df = pd.read_csv(fd_rq_path, delimiter=';')
                if not df.empty:
                    for param, value in df.iloc[0].dropna().to_dict().items():
                        current_dataset_results["rangeQueryResults"]["original"].append({'parameter': param, 'value': float(value)})
            
            # Training Set Range Query FD
            path_in_main_fd_dir = os.path.join(PARENT_DIRS['fractalDimension'], f"fd_rqR_{dataset_id}_ts*.csv")
            path_in_ts_dir_pattern = os.path.join(PARENT_DIRS['trainingSets'], dataset_id, '**', f"fd_rqR_{dataset_id}_ts*.csv")
            print(f"DEBUG: Sto cercando i file del training set in: {path_in_ts_dir_pattern}")

            ts_files_main = glob.glob(path_in_main_fd_dir)
            ts_files_recursive = glob.glob(path_in_ts_dir_pattern, recursive=True)
            ts_files = sorted(ts_files_main + ts_files_recursive)

            for f_path in ts_files:
                filename = os.path.basename(f_path)
                source_name = ""
                training_sets_base_path = os.path.join(PARENT_DIRS['trainingSets'], dataset_id)

                if f_path.startswith(training_sets_base_path):
                    try:
                        relative_path = os.path.relpath(f_path, training_sets_base_path)
                        source_name = relative_path.split(os.sep)[0]
                    except ValueError:
                         source_name = "Training Set (Errore Percorso)"
                else:
                    match = re.search(r'_ts(\d*)\.csv', filename)
                    ts_number = match.group(1) if match and match.group(1) else ''
                    source_name = f"Training Set {ts_number}".strip()

                ts_data = []
                df = pd.read_csv(f_path, delimiter=';')
                if not df.empty:
                    for param, value in df.iloc[0].dropna().to_dict().items():
                        ts_data.append({'parameter': param, 'value': float(value)})
                
                if ts_data:
                    current_dataset_results["rangeQueryResults"]["trainingSets"].append({
                        "source": source_name,
                        "data": ts_data
                    })
        except Exception as e:
             logging.error(f"Error processing Range Query Results for {dataset_id}: {e}")

        # Add the collected results for this dataset_id to the main dictionary
        results_by_dataset[dataset_id] = current_dataset_results

    return jsonify(results_by_dataset)


# ==============================================================================
# --- MAIN EXECUTION BLOCK ---
# ==============================================================================

if __name__ == '__main__':
    # Read ambient variable 'PORT', if not defined use '5000' as default
    port = int(os.environ.get('PORT', 5000))
    # Starts the Flask-SocketIO development server.
    socketio.run(app, host='0.0.0.0', port=port, debug=True)
